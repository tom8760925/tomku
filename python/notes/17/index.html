<!doctype html><html lang=en-us dir=ltr><head><meta charset=utf-8><meta name=viewport content="width=device-width"><title>python語言的筆記-tensorflow-文字AI模型 | tomku的網誌
</title><link rel=stylesheet href=/tomku/css/cssall.min.6db1d46d747ddea4cd158fef8a0e6f577651a77ebbe8bf96fc205497064d9dcf.css><script src=/tomku/js/main.js></script><script src=/tomku/js/header.js></script></head><body><header class=baseof_header_class><nav class=header_nav_class><div id=header_div_left class=header_div_class><a href=/tomku/><h1 class=header_h1_class>tomku的網誌</h1></a></div><div id=header_div_right class=header_div_class><ul class=header_menu_ul_class><li class=header_menu_li_class><a class=header_menu_a_class href=/tomku/>首頁</a></li><li class=header_menu_li_class><a class=header_menu_a_class2 onclick='displaymenu("header_menu_li_class_1")'>程式</a><ul id=header_menu_li_class_1 class=header_menu_ul_class2><li class=header_menu_li_class2><a class=header_menu_a_class href=/tomku/c/>C</a></li><li class=header_menu_li_class2><a class=header_menu_a_class href=/tomku/c++/>C++</a></li><li class=header_menu_li_class2><a class=header_menu_a_class href=/tomku/golang/>golang</a></li><li class=header_menu_li_class2><a class=header_menu_a_class href=/tomku/java/>java</a></li><li class=header_menu_li_class2><a class=header_menu_a_class href=/tomku/python/>python</a></li><li class=header_menu_li_class2><a class=header_menu_a_class href=/tomku/csharp/>csharp</a></li></ul></li><li class=header_menu_li_class><a class=header_menu_a_class2 onclick='displaymenu("header_menu_li_class_2")'>其他</a><ul id=header_menu_li_class_2 class=header_menu_ul_class2><li class=header_menu_li_class2><a class=header_menu_a_class href=/tomku/esp/>esp</a></li><li class=header_menu_li_class2><a class=header_menu_a_class href=/tomku/git/>git</a></li><li class=header_menu_li_class2><a class=header_menu_a_class href=/tomku/hugo/>hugo</a></li><li class=header_menu_li_class2><a class=header_menu_a_class href=/tomku/mysql/>mysql</a></li><li class=header_menu_li_class2><a class=header_menu_a_class href=/tomku/minecraft/>minecraft</a></li><li class=header_menu_li_class2><a class=header_menu_a_class href=/tomku/qt/>qt</a></li><li class=header_menu_li_class2><a class=header_menu_a_class href=/tomku/linux/>linux</a></li><li class=header_menu_li_class2><a class=header_menu_a_class href=/tomku/other/>other</a></li></ul></li><li class=header_menu_li_class><a class=header_menu_a_class href=/tomku/tags/>標籤</a></li><li class=header_menu_li_class><a class=header_menu_a_class href=/tomku/about/>關於</a></li></ul></div></nav></header><main class=baseof_main_class><aside class="main left_aside_class"><div class=left_div_class3><button class=left_button_class onclick=displayleft()>☰</button></div><div id=left_div_class4 class=left_div_class4><div class=left_div_class><div class=left_div_class2>個人連結</div><div class=left_social_div_class><ul class=left_social_ul_class><li class=left_social_li_class><a href=https://github.com/tom8760925 class=left_social_a_class>github</a></li><li class=left_social_li_class><a href=https://www.notion.so/tomku-1dafd63b811e80debc8fe145cc6d494a class=left_social_a_class>副網誌</a></li></ul></div></div><div class=left_div_class><div class=left_div_class2>最新文章</div><nav><ul class=left_recent_ul_class><li class=left_recent_li_class><article><header class=left_recent_header_class><a href=/tomku/python/notes/19/>python語言的筆記-pytorch-文字AI模型</a></header><footer class=left_recent_footer_class><time datetime=2025-10-18T15:07:01+08:00>2025-10-18</time></footer><a class=left_recent_a_class href=/tomku/python/notes/19/></a></article></li><li class=left_recent_li_class><article><header class=left_recent_header_class><a href=/tomku/python/notes/18/>python語言的筆記-pytorch-圖像AI模型</a></header><footer class=left_recent_footer_class><time datetime=2025-09-30T17:24:43+08:00>2025-09-30</time></footer><a class=left_recent_a_class href=/tomku/python/notes/18/></a></article></li><li class=left_recent_li_class><article><header class=left_recent_header_class><a href=/tomku/ai/no/08/>AI的練習-深度學習</a></header><footer class=left_recent_footer_class><time datetime=2025-09-08T15:56:10+08:00>2025-09-08</time></footer><a class=left_recent_a_class href=/tomku/ai/no/08/></a></article></li><li class=left_recent_li_class><article><header class=left_recent_header_class><a href=/tomku/ai/no/07/>AI的練習-其他</a></header><footer class=left_recent_footer_class><time datetime=2025-09-08T14:01:40+08:00>2025-09-08</time></footer><a class=left_recent_a_class href=/tomku/ai/no/07/></a></article></li><li class=left_recent_li_class><article><header class=left_recent_header_class><a href=/tomku/python/practice/focus14/>python語言的練習-練習實作重點14-變數範圍</a></header><footer class=left_recent_footer_class><time datetime=2025-09-08T13:33:10+08:00>2025-09-08</time></footer><a class=left_recent_a_class href=/tomku/python/practice/focus14/></a></article></li></ul></nav></div><div class=left_div_class><div class=left_div_class2>標籤</div><div class=left_tags_div_class><a href=/tomku/categories/ai/ class=left_tags_a_class>Ai<sup>10</sup>
</a><a href=/tomku/categories/c++%E8%AA%9E%E8%A8%80/ class=left_tags_a_class>C++語言<sup>169</sup>
</a><a href=/tomku/categories/cmake/ class=left_tags_a_class>Cmake<sup>2</sup>
</a><a href=/tomku/categories/csharp%E8%AA%9E%E8%A8%80/ class=left_tags_a_class>Csharp語言<sup>1</sup>
</a><a href=/tomku/categories/c%E8%AA%9E%E8%A8%80/ class=left_tags_a_class>C語言<sup>22</sup>
</a><a href=/tomku/categories/esp/ class=left_tags_a_class>Esp<sup>9</sup>
</a><a href=/tomku/categories/git/ class=left_tags_a_class>Git<sup>1</sup>
</a><a href=/tomku/categories/golang%E8%AA%9E%E8%A8%80/ class=left_tags_a_class>Golang語言<sup>20</sup>
</a><a href=/tomku/categories/hugo/ class=left_tags_a_class>Hugo<sup>2</sup>
</a><a href=/tomku/categories/linux/ class=left_tags_a_class>Linux<sup>30</sup>
</a><a href=/tomku/categories/minecraft/ class=left_tags_a_class>Minecraft<sup>6</sup>
</a><a href=/tomku/categories/mysql/ class=left_tags_a_class>Mysql<sup>3</sup>
</a><a href=/tomku/categories/proxomx/ class=left_tags_a_class>Proxomx<sup>3</sup>
</a><a href=/tomku/categories/python%E8%AA%9E%E8%A8%80/ class=left_tags_a_class>Python語言<sup>34</sup>
</a><a href=/tomku/categories/qt/ class=left_tags_a_class>Qt<sup>11</sup>
</a><a href=/tomku/categories/xcode/ class=left_tags_a_class>Xcode<sup>3</sup>
</a><a href=/tomku/categories/%E5%85%B6%E4%BB%96/ class=left_tags_a_class>其他<sup>1</sup>
</a><a href=/tomku/categories/%E9%97%9C%E6%96%BC/ class=left_tags_a_class>關於<sup>1</sup></a></div></div></div></aside><div class="main main_div_class"><section class=main_page_section_class><article><header><div class=main_page_breadcrumb_div_class><a href=/tomku/>簡介
</a>/<a href=/tomku/python/>Pythons
</a>/<a href=/tomku/python/notes/>筆記
</a>/</div><h1 class=main_page_h1_class>python語言的筆記-tensorflow-文字AI模型</h1></header><time class=main_page_time_class datetime=2025-08-30T14:49:32+08:00>2025-08-30</time><div class=main_page_div_class2><h1 id=tensorflow-文字ai模型>tensorflow-文字AI模型</h1><p>這篇是python的筆記第17篇</p><h2 id=對稱式>對稱式</h2><h3 id=simplernn>SimpleRNN</h3><p>這是rnn的稍微進化版，修改rnn的問題</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow <span style=color:#f92672>import</span> keras
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.datasets <span style=color:#f92672>import</span> imdb
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras <span style=color:#f92672>import</span> models
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.layers <span style=color:#f92672>import</span> Embedding, SimpleRNN, Dense
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.preprocessing <span style=color:#f92672>import</span> sequence
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- 1. 資料載入與預處理 ---</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 設定參數</span>
</span></span><span style=display:flex><span>MAX_FEATURES <span style=color:#f92672>=</span> <span style=color:#ae81ff>10000</span>  <span style=color:#75715e># 只使用數據集中最常出現的 10000 個詞彙</span>
</span></span><span style=display:flex><span>MAX_LEN <span style=color:#f92672>=</span> <span style=color:#ae81ff>500</span>         <span style=color:#75715e># 每條評論最大長度，超過會截斷，不足會補零 (padding)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;載入 IMDB 資料集...&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 載入IMDB 資料集10000 個詞彙</span>
</span></span><span style=display:flex><span>(X_train, y_train), (X_test, y_test) <span style=color:#f92672>=</span> imdb<span style=color:#f92672>.</span>load_data(num_words<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 將所有評論長度都變成500</span>
</span></span><span style=display:flex><span>X_train <span style=color:#f92672>=</span> sequence<span style=color:#f92672>.</span>pad_sequences(X_train, maxlen<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>)
</span></span><span style=display:flex><span>X_test <span style=color:#f92672>=</span> sequence<span style=color:#f92672>.</span>pad_sequences(X_test, maxlen<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 訓練模型</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> models<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>    [
</span></span><span style=display:flex><span>    <span style=color:#75715e># 詞嵌入層，詞彙表大小 (10000)，詞向量維度 (32)，# 輸入序列長度 (500)</span>
</span></span><span style=display:flex><span>    Embedding(input_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span>,output_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>, input_length<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>),
</span></span><span style=display:flex><span>    <span style=color:#75715e># SimpleRNN 層，定義隱藏層的神經元數量</span>
</span></span><span style=display:flex><span>    SimpleRNN(units<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>),
</span></span><span style=display:flex><span>    <span style=color:#75715e># 輸出層 將輸出壓縮到 0 到 1 之間</span>
</span></span><span style=display:flex><span>    Dense(units<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;sigmoid&#39;</span>)
</span></span><span style=display:flex><span>    ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 編譯：設定優化器、損失函數和評估指標</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>compile(optimizer<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;adam&#39;</span>,
</span></span><span style=display:flex><span>              loss<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;binary_crossentropy&#39;</span>,
</span></span><span style=display:flex><span>              metrics<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;accuracy&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 顯示模型架構</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>summary()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 訓練模型</span>
</span></span><span style=display:flex><span>history <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>fit(X_train, y_train,
</span></span><span style=display:flex><span>                    epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>                    batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>,
</span></span><span style=display:flex><span>                    validation_split<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>, 
</span></span><span style=display:flex><span>                    verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;在測試集上評估模型...&#34;</span>)
</span></span><span style=display:flex><span>loss, accuracy <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>evaluate(X_test, y_test, verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;測試集上的 Loss: </span><span style=color:#e6db74>{</span>loss<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;測試集上的 Accuracy: </span><span style=color:#e6db74>{</span>accuracy<span style=color:#f92672>*</span><span style=color:#ae81ff>100</span><span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>%&#34;</span>)
</span></span></code></pre></div><h3 id=lstm>LSTM</h3><p>rnn的進化版</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow <span style=color:#f92672>import</span> keras
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.datasets <span style=color:#f92672>import</span> imdb
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras <span style=color:#f92672>import</span> models
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.layers <span style=color:#f92672>import</span> Embedding, LSTM, Dense
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.preprocessing <span style=color:#f92672>import</span> sequence
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># --- 1. 資料載入與預處理 ---</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 設定參數</span>
</span></span><span style=display:flex><span>MAX_FEATURES <span style=color:#f92672>=</span> <span style=color:#ae81ff>10000</span>  <span style=color:#75715e># 只使用數據集中最常出現的 10000 個詞彙</span>
</span></span><span style=display:flex><span>MAX_LEN <span style=color:#f92672>=</span> <span style=color:#ae81ff>500</span>         <span style=color:#75715e># 每條評論最大長度，超過會截斷，不足會補零 (padding)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#39;載入 IMDB 資料集...&#39;</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 載入IMDB 資料集10000 個詞彙</span>
</span></span><span style=display:flex><span>(X_train, y_train), (X_test, y_test) <span style=color:#f92672>=</span> imdb<span style=color:#f92672>.</span>load_data(num_words<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span>)
</span></span><span style=display:flex><span><span style=color:#75715e># 將所有評論長度都變成500</span>
</span></span><span style=display:flex><span>X_train <span style=color:#f92672>=</span> sequence<span style=color:#f92672>.</span>pad_sequences(X_train, maxlen<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>)
</span></span><span style=display:flex><span>X_test <span style=color:#f92672>=</span> sequence<span style=color:#f92672>.</span>pad_sequences(X_test, maxlen<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 訓練模型</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> models<span style=color:#f92672>.</span>Sequential(
</span></span><span style=display:flex><span>    [
</span></span><span style=display:flex><span>    <span style=color:#75715e># 詞嵌入層，詞彙表大小 (10000)，詞向量維度 (32)，# 輸入序列長度 (500)</span>
</span></span><span style=display:flex><span>    Embedding(input_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>10000</span>,output_dim<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>, input_length<span style=color:#f92672>=</span><span style=color:#ae81ff>500</span>),
</span></span><span style=display:flex><span>    <span style=color:#75715e># SimpleRNN 層，定義隱藏層的神經元數量</span>
</span></span><span style=display:flex><span>    LSTM(units<span style=color:#f92672>=</span><span style=color:#ae81ff>32</span>),
</span></span><span style=display:flex><span>    <span style=color:#75715e># 輸出層 將輸出壓縮到 0 到 1 之間</span>
</span></span><span style=display:flex><span>    Dense(units<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;sigmoid&#39;</span>)
</span></span><span style=display:flex><span>    ])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 編譯：設定優化器、損失函數和評估指標</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>compile(optimizer<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;adam&#39;</span>,
</span></span><span style=display:flex><span>              loss<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;binary_crossentropy&#39;</span>,
</span></span><span style=display:flex><span>              metrics<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;accuracy&#39;</span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 顯示模型架構</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>summary()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 訓練模型</span>
</span></span><span style=display:flex><span>history <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>fit(X_train, y_train,
</span></span><span style=display:flex><span>                    epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>                    batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>,
</span></span><span style=display:flex><span>                    validation_split<span style=color:#f92672>=</span><span style=color:#ae81ff>0.2</span>, 
</span></span><span style=display:flex><span>                    verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>&#34;在測試集上評估模型...&#34;</span>)
</span></span><span style=display:flex><span>loss, accuracy <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>evaluate(X_test, y_test, verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;測試集上的 Loss: </span><span style=color:#e6db74>{</span>loss<span style=color:#e6db74>:</span><span style=color:#e6db74>.4f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;測試集上的 Accuracy: </span><span style=color:#e6db74>{</span>accuracy<span style=color:#f92672>*</span><span style=color:#ae81ff>100</span><span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>%&#34;</span>)
</span></span></code></pre></div><h2 id=非對稱式>非對稱式</h2><h2 id=llmbert>LLM(BERT)</h2><p>大型語言模型，主要是先將文字拆分，再將文字進行編號、填充、段落標記，之後設計模型進行訓練判斷(主要是用依照需求設計，不太會用現有模型)</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-py data-lang=py><span style=display:flex><span><span style=color:#f92672>from</span> datasets <span style=color:#f92672>import</span> load_dataset
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> tensorflow <span style=color:#66d9ef>as</span> tf
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> transformers <span style=color:#f92672>import</span> BertTokenizer, TFBertModel
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.models <span style=color:#f92672>import</span> Sequential
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> tensorflow.keras.layers <span style=color:#f92672>import</span> Dense, Input, Dropout
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.model_selection <span style=color:#f92672>import</span> train_test_split
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model_name <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;bert-base-uncased&#39;</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 載入的 imdb </span>
</span></span><span style=display:flex><span>raw_datasets <span style=color:#f92672>=</span> load_dataset(<span style=color:#e6db74>&#34;imdb&#34;</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 將資料擷取200筆</span>
</span></span><span style=display:flex><span>train_slice <span style=color:#f92672>=</span> raw_datasets[<span style=color:#e6db74>&#39;train&#39;</span>]<span style=color:#f92672>.</span>select(range(<span style=color:#ae81ff>200</span>))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 設定資料集和標籤</span>
</span></span><span style=display:flex><span>texts <span style=color:#f92672>=</span> list(train_slice[<span style=color:#e6db74>&#39;text&#39;</span>])
</span></span><span style=display:flex><span>labels <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(train_slice[<span style=color:#e6db74>&#39;label&#39;</span>]) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 載入分詞器</span>
</span></span><span style=display:flex><span>tokenizer <span style=color:#f92672>=</span> BertTokenizer<span style=color:#f92672>.</span>from_pretrained(model_name)
</span></span><span style=display:flex><span><span style=color:#75715e># 載入模型並自動轉換格式並載入</span>
</span></span><span style=display:flex><span>bert_model <span style=color:#f92672>=</span> TFBertModel<span style=color:#f92672>.</span>from_pretrained(model_name, from_pt<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>) 
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 使用 BERT 分詞器，載入資料，設定框架輸出格式tf，設定所有字串長度都字串最常長度一樣不夠就補，設定字串最常長度128，只要長度超過就截斷</span>
</span></span><span style=display:flex><span><span style=color:#75715e># encoded_input 包含三個關鍵張量 (shape: 6x128):</span>
</span></span><span style=display:flex><span><span style=color:#75715e># - input_ids: 文本轉換後的數字 ID (包含 [CLS]:101, [SEP]:102, [PAD]:0)。</span>
</span></span><span style=display:flex><span><span style=color:#75715e># - attention_mask: 標記哪些是真實 Token (1)，哪些是填充 Token (0)。</span>
</span></span><span style=display:flex><span><span style=color:#75715e># - token_type_ids: 段落 ID，單句任務中全部為 0。</span>
</span></span><span style=display:flex><span>encoded_input <span style=color:#f92672>=</span> tokenizer(
</span></span><span style=display:flex><span>    texts,
</span></span><span style=display:flex><span>    return_tensors<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;tf&#39;</span>,
</span></span><span style=display:flex><span>    padding<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;max_length&#39;</span>,
</span></span><span style=display:flex><span>    truncation<span style=color:#f92672>=</span><span style=color:#66d9ef>True</span>,
</span></span><span style=display:flex><span>    max_length<span style=color:#f92672>=</span><span style=color:#ae81ff>128</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 執行模型</span>
</span></span><span style=display:flex><span>outputs <span style=color:#f92672>=</span> bert_model(encoded_input)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 輸出last_hidden_state</span>
</span></span><span style=display:flex><span>X_bert <span style=color:#f92672>=</span> outputs[<span style=color:#e6db74>&#39;last_hidden_state&#39;</span>][:, <span style=color:#ae81ff>0</span>, :]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 轉換成numpy</span>
</span></span><span style=display:flex><span>X_embeddings <span style=color:#f92672>=</span> X_bert<span style=color:#f92672>.</span>numpy()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 將資料集分成訓練和測試</span>
</span></span><span style=display:flex><span>X_train, X_test, y_train, y_test <span style=color:#f92672>=</span> train_test_split(
</span></span><span style=display:flex><span>    X_embeddings, labels, test_size<span style=color:#f92672>=</span><span style=color:#ae81ff>0.3</span>, random_state<span style=color:#f92672>=</span><span style=color:#ae81ff>42</span>
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 訓練模型</span>
</span></span><span style=display:flex><span>model <span style=color:#f92672>=</span> Sequential([
</span></span><span style=display:flex><span>    <span style=color:#75715e># 設定輸入值768</span>
</span></span><span style=display:flex><span>    Input(shape<span style=color:#f92672>=</span>(<span style=color:#ae81ff>768</span>,)), 
</span></span><span style=display:flex><span>    <span style=color:#75715e># 全連接層 將攤平層整理出128的輸出</span>
</span></span><span style=display:flex><span>    Dense(<span style=color:#ae81ff>128</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;relu&#39;</span>),
</span></span><span style=display:flex><span>    <span style=color:#75715e># 0.2的機率隨機關閉輸出</span>
</span></span><span style=display:flex><span>    Dropout(<span style=color:#ae81ff>0.2</span>),
</span></span><span style=display:flex><span>    <span style=color:#75715e># 全連接層 最後整理輸出訓練集所分類的1個</span>
</span></span><span style=display:flex><span>    Dense(<span style=color:#ae81ff>1</span>, activation<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;sigmoid&#39;</span>)
</span></span><span style=display:flex><span>])
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 編譯：設定優化器、損失函數和評估指標</span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>compile(
</span></span><span style=display:flex><span>    optimizer<span style=color:#f92672>=</span>tf<span style=color:#f92672>.</span>keras<span style=color:#f92672>.</span>optimizers<span style=color:#f92672>.</span>Adam(learning_rate<span style=color:#f92672>=</span><span style=color:#ae81ff>1e-4</span>),
</span></span><span style=display:flex><span>    loss<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;binary_crossentropy&#39;</span>,
</span></span><span style=display:flex><span>    metrics<span style=color:#f92672>=</span>[<span style=color:#e6db74>&#39;accuracy&#39;</span>]
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>model<span style=color:#f92672>.</span>summary()
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 開始訓練，訓練1次，將資料分成2</span>
</span></span><span style=display:flex><span>history <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>fit(
</span></span><span style=display:flex><span>    X_train,
</span></span><span style=display:flex><span>    y_train,
</span></span><span style=display:flex><span>    epochs<span style=color:#f92672>=</span><span style=color:#ae81ff>1</span>,
</span></span><span style=display:flex><span>    batch_size<span style=color:#f92672>=</span><span style=color:#ae81ff>2</span>,
</span></span><span style=display:flex><span>    validation_data<span style=color:#f92672>=</span>(X_test, y_test)
</span></span><span style=display:flex><span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e># 評估模型</span>
</span></span><span style=display:flex><span>loss, accuracy <span style=color:#f92672>=</span> model<span style=color:#f92672>.</span>evaluate(X_test, y_test, verbose<span style=color:#f92672>=</span><span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>print(<span style=color:#e6db74>f</span><span style=color:#e6db74>&#34;測試集準確度: </span><span style=color:#e6db74>{</span>accuracy<span style=color:#e6db74>:</span><span style=color:#e6db74>.2f</span><span style=color:#e6db74>}</span><span style=color:#e6db74>&#34;</span>)
</span></span></code></pre></div></div><footer><div><ul class=main_page_terms_ul_class><li class=main_page_terms_li_class><div>標籤:</div></li><ul class=main_page_terms_ul_class2><li class=main_page_terms_li_class2><div class=main_page_terms_div_class>Python語言</div><a class=main_page_terms_a_class href=/tomku/tags/python%E8%AA%9E%E8%A8%80/></a></li><li class=main_page_terms_li_class2><div class=main_page_terms_div_class>筆記</div><a class=main_page_terms_a_class href=/tomku/tags/%E7%AD%86%E8%A8%98/></a></li><li class=main_page_terms_li_class2><div class=main_page_terms_div_class>Tensorflow</div><a class=main_page_terms_a_class href=/tomku/tags/tensorflow/></a></li><li class=main_page_terms_li_class2><div class=main_page_terms_div_class>AI</div><a class=main_page_terms_a_class href=/tomku/tags/ai/></a></li></ul></ul></div><nav class=main_page_section_nav_class><a class=main_page_section_a_class href=/tomku/python/notes/15/><samp>上一頁</samp><br><samp>python語言的筆記-tensorflow-圖像AI</samp><br><samp><time datetime=2025-08-26T13:50:59+08:00>2025-08-26</time>
</samp></a><a class=main_page_section_a_class2 href=/tomku/python/notes/18/><samp>下一頁</samp><br><samp>python語言的筆記-pytorch-圖像AI模型</samp><br><samp><time datetime=2025-09-30T17:24:43+08:00>2025-09-30</time></samp></a></nav></footer></article></section></div><aside class="main right_aside_class"><div class=right_div_class><details class=right_details_class open><summary class=right_details_class>目錄</summary><nav id=TableOfContents><ul><li><a href=#tensorflow-文字ai模型>tensorflow-文字AI模型</a><ul><li><a href=#對稱式>對稱式</a><ul><li><a href=#simplernn>SimpleRNN</a></li><li><a href=#lstm>LSTM</a></li></ul></li><li><a href=#非對稱式>非對稱式</a></li><li><a href=#llmbert>LLM(BERT)</a></li></ul></li></ul></nav></details></div><div class=right_div_class><details class=right_related_details_class open><summary class=right_related_details_class>相關文章</summary><nav class=right_related_page_nav_class><ol class=right_related_page_ol_class></ol><hr></nav></details></div></aside></main><footer class=baseof_footer_class><center><p>Copyright @2024-2025 tomku的網誌 Powered by <a href=https://gohugo.io/>Hugo</a></p><p xmlns:cc=http://creativecommons.org/ns# xmlns:dct=http://purl.org/dc/terms/>Except where otherwise noted, <span property="dct:title">tomku的網誌</span> by <span property="cc:attributionName">YU-LUN KU</span> is licensed under <a href="http://creativecommons.org/licenses/by-sa/4.0/?ref=chooser-v1" target=_blank rel="license noopener noreferrer" style=display:inline-block>Attribution-ShareAlike 4.0 International<img style=height:22px!important;margin-left:3px;vertical-align:text-bottom;display:inline-block src="https://mirrors.creativecommons.org/presskit/icons/cc.svg?ref=chooser-v1"><img style=height:22px!important;margin-left:3px;vertical-align:text-bottom;display:inline-block src="https://mirrors.creativecommons.org/presskit/icons/by.svg?ref=chooser-v1"><img style=height:22px!important;margin-left:3px;vertical-align:text-bottom;display:inline-block src="https://mirrors.creativecommons.org/presskit/icons/sa.svg?ref=chooser-v1"></a></p><p>若有需聯絡請寄信到tom8760925@gmail.com</p><p>若分享內容有侵害您的著作權，請來信告知，我將儘速移除相關內容</p></center></footer></body></html>